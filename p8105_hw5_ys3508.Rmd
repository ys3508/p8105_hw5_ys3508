---
title: "p8105_hw5_ys3508"
output: github_document
date: "2022-11-16"
---
# Writing Function
```{r}
library(tidyverse)
library(rvest)
library (readr)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```


## Problem 1
### Create a tidy dataframe
```{r}
file_names = list.files(path = "data/problem1", pattern = NULL, all.files = FALSE, full.names = TRUE)
```

Iterate over file names and read in data for each subject using purrr::map and saving the result as a new variable in the dataframe
```{r}
#using map + read_csv function
hw1 = map(file_names, read_csv)

for (i in 1:20) 
  {
  
  if (i < 11) {
    
    hw1 [[i]] = hw1 [[i]] %>% 
      mutate(
        subject_id = i,
        arm = "Control")
  }
  
  else {
    
    hw1 [[i]] = hw1 [[i]] %>% 
      mutate(
        subject_id = i - 10,
        arm = "Experimental")
  }
  
}

```
Tidy the result

```{r}
hw1_tidy = hw1 %>%
  bind_rows() %>% 
  pivot_longer(
    week_1:week_8,
    names_to = "week",
    names_prefix = "week_",
    values_to = "observation"
  ) %>%  
  mutate(
    subject_id = as.character(subject_id),
    week = as.numeric(week)
  ) %>% 
  select(subject_id, arm, everything())
 
```

### Plot

```{r}
hw1_tidy %>% 
  ggplot(aes(x = week, y = observation, group = subject_id, colour = subject_id)) +
    labs(x = "Time in week", y = "Observations", color = "Control arm", title = "Observations on each subject over time") + 
    geom_line(size=0.5) +
    geom_point(size=2) +
    facet_grid(. ~ arm) +
    theme_bw() +
    theme(legend.position="bottom")

```

* The observation values for individuals in the experimental arm are greater on average than those in the control arm. Furthermore, there appears to be some growth in a fairly linear fashion over time for those in the experimental arm, but the observation value for those in the control arm tends to be more steady.

## Problem 2
### Import data
```{r}
urlfile="https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv"

hw2 <-read_csv(url(urlfile))
```

* The 'hw2' dataset comprises 12 variables with a total of 52179 observations, detailing murder cases in various cities around the United States. These factors include the reported date and location, some victim information (name, race, age, and gender), and the homicide's ultimate disposition.

### Create a city_state variable/Summary

```{r}
hw2_homicides = hw2 %>% 
 mutate(
    state = str_to_upper(state),
    city_state = str_c(city, ", ", state),
    city_state = recode(city_state, "Tulsa, AL" = "Tulsa, OK"), #error in dataset;
    final_disposition =
          ifelse(disposition == "Closed by arrest" ,"solved", "unsolved")) 

 

hw2_summary = hw2_homicides %>%
  group_by(city_state) %>% 
  summarise(
    total_homicides = n(),
    unsolved_homicides = sum(final_disposition == "unsolved")
  ) %>% 
  knitr::kable(
    col.names = c("City, State", "Total Number of Homicides", "Number of Unsolved Homicides")
  )
hw2_summary
```

###  `prop.test` function for MD

```{r}

hw2_md = hw2_homicides %>% 
  filter(city_state == "Baltimore, MD") 

prop_test = prop.test(1825,2827, conf.level=0.95, correct=FALSE)

broom::tidy(prop_test)

prop_test[["estimate"]]
prop_test[["conf.int"]]

```

* Using `prop_test[["estimate"]]` and `prop_test[["conf.int"]]` to get the estimate of 0.6455607 and confidence intervals from 0.6275625 to 0.6631599.

### Using `map` for `prop.test`
define the `prop_test` function
```{r}
    
prop_test = function(dataset) {
  
  prop_test = dataset %>%
  summarise(
    total_homicides = n(),
    unsolved_homicides = sum(final_disposition == "unsolved")
  )
  
  test = prop.test(
      prop_test$unsolved_homicides, 
      prop_test$total_homicides, 
      conf.level = 0.95, correct=FALSE
      ) %>% 
    broom::tidy()
   
  test
}

```

use `nest` and `map`
```{r}

  
hw2_nest = hw2_homicides %>%
  select(city_state, everything()) %>% 
  nest(data = uid:final_disposition) 

hw2_unsolved = hw2_nest %>% 
   mutate(test_result = map(data, prop_test)) %>% 
   unnest(test_result) 

hw2_unsolved%>% 
  select(city_state, estimate, conf.low, conf.high) %>% 
   knitr::kable(digits = 3)


```

### Plot
```{r}
hw2_unsolved %>% 
  mutate(
    city_state = fct_reorder(city_state, estimate)) %>% 
ggplot(aes(x=city_state, y=estimate, color =city_state)) + 
  geom_point() +
  geom_errorbar(aes(ymin=conf.low, ymax=conf.high), width=.2,
                 position=position_dodge(.9))+
  labs(title="The estimates and CIs for proportion of unsolved homicides for each city", x="City and state", y = "Estimtate")
```

## Problem 3

`t_test` function:

```{r}

t_test = function(n = 30, mu, sigma = 5) {
  
  x = rnorm(n, mean = mu, sd = sigma)
  
  t_test =
    t.test(x, mu = 0) %>% 
    broom::tidy() 
  
  t_test %>% 
    select(estimate, p.value)
  
}

```

Generate 5000 datasets from the model:

```{r}
sim_results =
  rerun(5000, t_test(mu = 0)) %>%  #generate 5000 dataset;
  bind_rows()
sim_results

```

Repeat the above for μ={1,2,3,4,5,6}:

```{r}
sim_results_df = 
  expand_grid(
    mu = c(1, 2, 3, 4, 5, 6),
    iteration = 1:5000
  ) %>% 
  mutate(
    estimate_df = map(.x = mu, ~ t_test (mu = .x))
  ) %>% 
  unnest(estimate_df)
sim_results_df
```

Make a plot showing the proportion of times the null was rejected (the power of the test) on the y axis and the true value of μ on the x axis. Describe the association between effect size and power.
